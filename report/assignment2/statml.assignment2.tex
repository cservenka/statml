\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{a4wide}
\usepackage{dsfont}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}

\setlength{\parindent}{0pt}

\title{\textbf{Assignment 2} \\ \small Statistical Methods for Machine Learning }
\author{\textbf{Group members}\\
        Ásbjørn Viderø Jøkladal\\
        Martin Holm Cservenka\\
        Tue Haulund}
\date{3\textsuperscript{rd} of March, 2015}		

%=======================================================================
\begin{document}
\maketitle
\tableofcontents
\newpage
%=======================================================================

\section{Classification}

\subsection{Linear discriminant analysis}
Upon execution of the implemented linear discriminant analysis of the training- and test-set, the following accuracy results are obtained
\begin{figure}[H]
	\begin{lstlisting}
	Accuracy of LDA on training dataset: 0.86
	Accuracy of LDA on test dataset: 0.815789473684
	\end{lstlisting}
	\caption{Training and test accuracy results of the implemented LDA algorithm}
	\label{fig:lda_results}
\end{figure}

\subsection{LDA and normalization}
After normalization of the training and test dataset, the following accuracy results are obtained after executing the linear discriminant analysis on them
\begin{figure}[H]
	\begin{lstlisting}
	Training set, feature = 0: Mean = -3.46833672893e-15, variance = 1.0
	Training set, feature = 1: Mean = -1.81743509131e-15, variance = 1.0
	Test set, feature = 0: Mean = 0.208375768039, variance = 1.07339795338
	Test set, feature = 1: Mean = 0.432138257729, variance = 1.25222270424
	Training set: Accuracy = 0.86
	Test set: Accuracy = 0.815789473684
	\end{lstlisting}
	\caption{Mean, variances and accuracies of normalized data sets}
	\label{fig:lda_norm_results}
\end{figure}

% TODO: Describe why accuracy does not change after normalization
As shown in figure~\ref{fig:lda_results} and figure~\ref{fig:lda_norm_results}, the accuracies have not changed after normalization of the dataset, because reasons. 

\subsection{Bayes optimal classification and probabilistic classification}
Since the hypothesis of a classifier is a (deterministic) function, there are only two possibilities for selecting our classifier: Given the only input element $0$, the classifier must either always return $0$ or always return $1$. Clearly, the Bayes optimal classifier is the one that always returns $1$. Assuming 0-1 loss, the risk of this classifier is $E[1 \{ h(X) \neq Y \}] = P(h(X) \neq Y) = 0.25$.\\

If we again assume 0-1 loss, the risk of the probabilistic classifier is
\begin{align*}
E[1 \{ h(X) \neq Y \}] &= P(h(X) \neq Y) \\
&= P((h(X) = 0, Y = 1) \cup (h(X) = 1, Y = 0)) \\
&= P(h(X) = 0, Y = 1) + P(h(X) = 1, Y = 0) \\
&= P(h(X) = 0) \cdot P(Y = 1) + P(h(X) = 1) \cdot P(Y = 0) \\
&= 0.25 \cdot 0.75 + 0.75 \cdot 0.25 = 0.375
\end{align*}
where we have used the fact that the pairs of events are disjoint (to get from line 2 to 3) and that the events in both pairs are independent (to get from line 3 to 4). We see that this probabilistic classifier has higher risk than the deterministic one that simply always returns 1.

\section{ Regression: Sunspot Prediction}

\subsection{Maximum likelihood solution}

\subsection{Maximum a posteriori solution}

\subsection{Weighted sum-of-squares}

\end{document}
